{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Languages of Data Science\n",
    "\n",
    "\n",
    "The languages of Data Science For anyone just getting started on their data science journey, the range of technical options can be overwhelming. There is a dizzying amount of choice when it comes to programming languages. Each has it's own strengths and weaknesses and **there is no one right answer to the question of which one you should learn first. The answer to that question depends largely on your needs, the problems you are trying to solve, and who you are solving them for**. \n",
    "\n",
    "**Python, R, and SQL are the languages that we recommend you consider first and foremost**. But there are so many others that have their own strengths and features. Scala, Java, C++, and Julia are some of the most popular. Javascript, PHP, Go, Ruby, and Visual Basic all have their own unique use cases as well. \n",
    "\n",
    "The language you choose to learn will depend on the things you need to accomplish and the problems you need to solve. It will also depend on what company you work for, what role you have, and the age of your existing application. We’ll explore the answers to this question as we dive into the popular languages in the data science industry. \n",
    "\n",
    "There are many roles available for people who are interested in getting involved in data science. Business Analyst Database Engineer Data Analyst Data Engineer Data Scientist Research Scientist Software Engineer Statistician Product Manager Project Manager and many more. \n",
    "\n",
    "Let’s dive into what we will learn in Lesson 1. We will put most of our focus on the **top three Data Science languages: Python, R, and SQL, which each have their own lessons**. Then we will go on to highlight other noteworthy languages and what makes them special. Then we’ll finish with a short quiz to test your knowledge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Python\n",
    "\n",
    "It is by far the most popular programming language for data science. According to the 2019 Kaggle Data Science and Machine Learning Survey, 75% of the over 10,000 respondents from around the world reported that they use Python on a regular basis. Glassdoor reported that in 2019 more than 75% of data science positions listed included Python in their job descriptions. When asked which language an aspiring data scientist should learn first, most data scientists say Python.\n",
    "\n",
    "1. Python is a high-level general-purpose programming language that can be applied to many different classes of problems.\n",
    "2. It has a large, standard library that provides tools suited to many different tasks, including but not limited to databases, automation, web scraping, text processing, image processing, machine learning, and data analytics.\n",
    "3. **For data science, you can use Python's scientific computing libraries such as `Pandas`, `NumPy`, `SciPy`, and `Matplotlib`.\n",
    "4. **For artificial intelligence, it has `TensorFlow`, `PyTorch`, `Keras`, and `Scikit-learn`**.\n",
    "5. Python can also be used for Natural Language Processing (NLP) using the Natural Language Toolkit (NLTK). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to R Language\n",
    "\n",
    "Like Python, R is free to use, but it's a GNU project - instead of being open source, it's actually free software. So if Python is open source and R is free software, what’s the difference? Well, Both open source and free software commonly refer to the same set of licenses. Many open source projects use the GNU General Public License, for example. Both open source and free software support collaboration. In many cases (but not all), these terms can be used interchangeably. The Open Source Initiative (OSI) champions open source while the Free Software Foundation (FSF) defines free software. \n",
    "Open source is more business focused, while free software is more focused on a set of values. \n",
    "\n",
    "Back to why you should learn R. Because this is a free software project, you can use the language in the same way that you contribute to open source, and it allows for public collaboration and private and commercial use. \n",
    "\n",
    "Plus, R is another language supported by a wide global community of people passionate about making it possible to use the language to solve big problems. \n",
    "\n",
    "Who is R for? It's most often used by statisticians, mathematicians, and data miners for developing statistical software, graphing, and data analysis. The language’s array-oriented syntax makes it easier to translate from math to code, especially for someone with no or minimal programming background. According to Kaggle’s Data Science and Machine Learning Survey, most folks learn R when they're a few years into their data science career, but it remains a welcoming language to those who don’t have a software programming background. \n",
    "\n",
    "R is popular in academia but companies that use R include IBM, Google, Facebook, Microsoft, Bank of America, Ford, TechCrunch, Uber, and Trulia. R has become the world’s largest repository of statistical knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to SQL\n",
    "\n",
    "SQL is a bit different from the other languages we’ve covered so far. First off, it's formally pronounced “ess cue el,” although some people say “sequel.” **While the acronym stands for “Structured Query Language,” many people do not consider SQL to be like other software development languages because it's a non-procedural language and its scope is limited to querying and managing data**. While it is not a “data science” language per se, data scientists regularly use it because it's simple and powerful! \n",
    "\n",
    "SQL was designed for managing data in relational databases.\n",
    "\n",
    "A relational database is formed by collections of two-dimensional tables; for example, datasets and Microsoft Excel spreadsheets. Each of these tables is then formed by a fixed number of columns and any number of rows. BUT! Even though SQL was originally developed for use with relational databases, because it's so pervasive and easy to use, SQL interfaces for many NoSQL and big data repositories have also been developed.\n",
    "\n",
    "When performing operations with SQL, you access the data directly. There's no need to copy it beforehand. This can speed up workflow executions considerably. SQL is the interpreter between you and the database. \n",
    "\n",
    "There are many different SQL databases available, including \n",
    "* MySQL\n",
    "* IBM Db2\n",
    "* PostgreSQL\n",
    "* Apache OpenOffice Base\n",
    "* SQLite\n",
    "* Oracle\n",
    "* MariaDB\n",
    "* Microsoft\n",
    "* SQL Server\n",
    "\n",
    "The syntax of the SQL you write might change a little bit based on the relational database management system you’re using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Languages\n",
    "\n",
    "Scala, Java, C++, and Julia are probably the most traditional data science languages on this slide. But JavaScript, PHP, Go, Ruby, Visual Basic, and others have all found their place in the data science community as well!\n",
    "\n",
    "### Java\n",
    "Java is a tried-and-true general-purpose object oriented programming language. It's been widely adopted in the enterprise space and is designed to be fast and scalable. Java applications are compiled to bytecode and run on the Java Virtual Machine, or \"JVM.\" Some notable data science tools built with Java include Weka, for data mining; Java-ML, which is a machine learning library; Apache MLlib, which makes machine learning scalable; and Deeplearning4j, for deep learning. Apache Hadoop is another Java-built application. It manages data processing and storage for big data applications running in clustered systems.\n",
    "\n",
    "### Scala\n",
    "Scala is a general-purpose programming language that provides support for functional programming and a strong static type system. Many of the design decisions in the construction of the Scala language were made to address criticisms of Java. Scala is also interoperable with Java, as it runs on the JVM. The name \"Scala\" is a combination of \"scalable\" and \"language.\" This language is designed to grow alongwith the demands of its users. For data science, the most popular program built using Scala is Apache Spark. Spark is a fast and general-purpose cluster computing system. It provides APIs that make parallel jobs easy to write, and an optimized engine that supports general computation graphs. Spark includes Shark, which is a query engine; MLlib, for machine learning; GraphX, for graph processing; and Spark Streaming. Apache Spark was designed to be faster than Hadoop..\n",
    "\n",
    "###  C++\n",
    "C++ is a general-purpose programming language. It is an extension of the C programming language, or \"C with Classes.” C++ improves processing speed, enables system programming, and provides broader control over the software application. Many organizations that use Python or other high-level languages for data analysis and exploratory tasks still rely on C++ to develop programs that feed that data to customers in real-time. For data science, a popular deep learning library for dataflow called TensorFlow was built with C++. But while C++ is the foundation of TensorFlow, it runs on a Python interface, so you don’t need to know C++ to use it. MongoDB, a NoSQL database for big data management, was built with C++. Caffe is a deep learning algorithm repository built with C++, with Python and MATLAB bindings.\n",
    "\n",
    "### Julia\n",
    "Julia was designed at MIT for high-performance numerical analysis and computational science. It provides speedy development like Python or R, while producing programs that run as fast as C or Fortran programs. Julia is compiled, which means that the code is executed directly on the processor as executable code; it calls C, Go, Java, MATLAB, R, Fortran, and Python libraries; and has refined parallelism. The Julia language is relatively new, having been written in 2012, but it has a lot of promise for future impact on the data science industry. JuliaDB is a particularly useful application of Julia for data science. It's a package for working with large persistent data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categories of Data Science Tools\n",
    "\n",
    "Open source tools are available for various data science tasks. We’ll have a look at the different data science tasks.  \n",
    "\n",
    "* **Data Management** is the process of persisting and retrieving data. \n",
    "* **Data Integration and Transformation**, often referred to as Extract, Transform, and Load, or “ETL,” is the process of retrieving data from remote data management systems. \n",
    "    - **Transforming data** and loading it into a local data management system is also part of Data Integration and Transformation. \n",
    "* **Data Visualization** is part of an initial data exploration process, as well as being part of a final deliverable.\n",
    "* **Model Building** is the process of creating a machine learning or deep learning model using an appropriate algorithm with a lot of data. \n",
    "* **Model deployment** makes such a machine learning or deep learning model available to third-party applications. \n",
    "* **Model monitoring and assessment** ensures continuous performance quality checks on the deployed models. These checks are for accuracy, fairness, and adversarial robustness. \n",
    "<br><br>\n",
    "- **Code asset management** uses versioning and other collaborative features to facilitate teamwork.\n",
    "- **Data asset management** brings the same versioning and collaborative components to data. Data asset management also supports replication, backup, and access right management. \n",
    "- **Development environments**, commonly known as Integrated Development Environments, or “IDEs”, are tools that help the data scientist to implement, execute, test, and deploy their work. \n",
    "- **Execution environments** are tools where data preprocessing, model training, and deployment take place. Finally, there is fully integrated, visual tooling available that covers all the previous tooling components, either partially or completely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Source Tools for Data Science\n",
    "\n",
    "### Data Management\n",
    "The most widely used open source data management tools are relational databases such as MySQL and [PostgreSQL](https://ru.wikipedia.org/wiki/PostgreSQL); NoSQL databases such as MongoDB Apache CouchDB, and [Apache Cassandra](https://ru.wikipedia.org/wiki/Apache_Cassandra); and file-based tools such as the [Hadoop File System](https://ru.wikipedia.org/wiki/Hadoop#HDFS) or Cloud File systems like Ceph. Finally,Elasticsearch is mainly used for storing text data and creating a search index for fast document retrieval\n",
    "\n",
    "### Data Integration and Transformation\n",
    "The most widely used open source data integration and transformation tools: \n",
    "* Apache AirFlow, originally created by AirBNB; \n",
    "* KubeFlow, which enables you to execute data science pipelines on top of Kubernetes;\n",
    "* Apache Kafka, which originated from LinkedIn; \n",
    "* Apache Nifi, which delivers a very nice visual editor; \n",
    "* Apache SparkSQL (which enables you to use ANSI SQL and scales up to compute clusters of 1000s of nodes); \n",
    "* NodeRED, which also provides a visual editor. NodeRED consumes so little in resources that it even runs on small devices like a Raspberry Pi. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization\n",
    "\n",
    "We have to distinguish between programming libraries where you need to use code and **tools that contain a user interface**. \n",
    "\n",
    "- Hue, which can create visualizations from SQL queries.\n",
    "- Kibana, a data exploration and visualization web application, is limited to Elasticsearch (the data provider). \n",
    "- Apache Superset is a data exploration and visualization web application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model deployment\n",
    "\n",
    "Model deployment is extremely important. Once you’ve created a machine learning model capable of predicting some key aspects of the future, you should make that model consumable by other developers and turn it into an API. \n",
    "\n",
    "- **Apache PredictionIO** currently only supports Apache Spark ML models for deployment, but support for all sorts of other libraries is on the roadmap. \n",
    "- **Seldon** is an interesting product since it supports nearly every framework, including TensorFlow, Apache SparkML, R, and scikit-learn. Seldon can run on top of Kubernetes and Redhat OpenShift. \n",
    "- **MLeap** Another way to deploy SparkML models is by using MLeap. \n",
    "- **TensorFlow service** TensorFlow can serve any of its models using the TensorFlow service. You can deploy to an embedded device like a Raspberry Pi or a smartphone using TensorFlow Lite, and even deploy to a web browser using TensorFlow dot JS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model monitoring and assessment\n",
    "\n",
    "Once you’ve deployed a machine learning model, you need to keep track of its prediction performance as new data arrives in order to maintain outdated models. Following are some examples of model monitoring tools: \n",
    "- **ModelDB** is a machine model metadatabase where information about the models are stored and can be queried. It natively supports Apache Spark ML Pipelines and scikit-learn. \n",
    "- A generic, multi-purpose tool called **Prometheus** is also widely used for machine learning model monitoring, although it’s not specifically made for this purpose. \n",
    "- Model performance is not exclusively measured through accuracy. Model bias against protected groups like gender or race is also important. The **IBM AI Fairness 360** open source toolkit does exactly this. It detects and mitigates against bias in machine learning models. Machine learning models, especially neural-network-based deep learning models, can be subject to adversarial attacks, where an attacker tries to fool the model with manipulated data or by manipulating the model itself. \n",
    "- The **IBM Adversarial Robustness 360 Toolbox** can be used to detect vulnerability to adversarial attacks and help make the model more robust. Machine learning modes are often considered to be a black box that applies some mysterious “magic.”\n",
    "- The **IBM AI Explainability 360 Toolkit** makes the machine learning process more understandable by finding similar examples within a dataset that can be presented to a user for manual comparison. The IBM AI Explainability 360 Toolkit can also illustrate training for a simpler machine learning model by explaining how different input variables affect the final decision of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code asset management\n",
    "\n",
    "Options for code asset management tools have been greatly simplified: \n",
    "* For code asset management – also referred to as version management or version control – **Git** is now the standard. Multiple services have emerged to support Git, with the most prominent being [GitHub](https://ru.wikipedia.org/wiki/GitHub), which provides hosting for software development version management. The runner-up is definitely [GitLab](https://ru.wikipedia.org/wiki/GitLab), which has the advantage of being a **fully open source platform that you can host and manage yourself**. Another choice is [Bitbucket](https://ru.wikipedia.org/wiki/Bitbucket)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data asset management\n",
    "\n",
    "Data asset management, also known as data governance or data lineage, is another crucial part of enterprise grade data science. Data has to be versioned and annotated with metadata. \n",
    "- **Apache Atlas** is a tool that supports this task. \n",
    "- Another interesting project, **ODPi Egeria**, is managed through the Linux Foundation and is an open ecosystem. It offers a set of open APIs, types, and interchange protocols that metadata repositories use to share and exchange data. \n",
    "- Finally, **Kylo** is an open source data lake management software platform that provides extensive support for a wide range of data asset management tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development Environments\n",
    "\n",
    "- One of the most popular current development environments that data scientists are using is **“Jupyter.”** Jupyter first emerged as a tool for interactive Python programming; it now supports more than a hundred different programming languages through “kernels.” Kernels shouldn’t be confused with operating system kernels. Jupyter kernels are encapsulating the different interactive interpreters for the different programming languages. A key property of Jupyter Notebooks is the ability to unify documentation, code, output from the code, shell commands, and visualizations into a single document.\n",
    "- **JupyterLab** is the next generation of Jupyter Notebooks and in the long term, will actually replace Jupyter Notebooks. The architectural changes being introduced in JupyterLab makes Jupyter more modern and modular. From a user’s perspective, the main difference introduced by JupyterLab is the ability to open different types of files, including Jupyter Notebooks, data, and terminals. You can then arrange these files on the canvas.\n",
    "- Although **Apache Zeppelin** has been fully reimplemented, it’s inspired by Jupyter Notebooks and provides a similar experience. One key differentiator is the integrated plotting capability. In Jupyter Notebooks, you are required to use external libraries in Apache Zeppelin, and plotting doesn’t require coding. You can also extend these capabilities by using additional libraries.\n",
    "- **RStudio** is one of the oldest development environments for statistics and data science, having been introduced in 2011. It exclusively runs R and all associated R libraries. However, Python development is possible and R is therefore tightly integrated into this tool to provide an optimal user experience. RStudio unifies programming, execution, debugging, remote data access, data exploration, and visualization into a single tool.\n",
    "- **Spyder** tries to mimic the behaviour of RStudio to bring its functionality to the Python world. Although Spyder does not have the same level of functionality as RStudio, data scientists do consider it an alternative. But in the Python world, Jupyter is used more frequently.\n",
    "- Sometimes your data doesn’t fit into a single computer’s storage or main memory capacity. That’s where cluster execution environments come in. The well known cluster-computing framework **Apache Spark** is among the most active Apache projects and is used across all industries, including in many Fortune 500 companies. The key property of Apache Spark is linear scalability. This means, if you double the number of servers in a cluster, you’ll also roughly double its performance.\n",
    "- After Apache Spark began to gain market share, **Apache Flink** was created. The key difference between Apache Spark and Apache Flink is that Apache Spark is a batch data processing engine, capable of processing huge amounts of data file by file. Apache Flink, on the other hand, is a stream processing image, with its main focus on processing real-time data streams. \n",
    "- One of the latest developments in the data science execution environments is called **“Ray,”** which has a clear focus on large-scale deep learning model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some integrated tools\n",
    "Let’s look at **open source tools for data scientists** that are fully integrated and visual. With these tools, **no programming knowledge is necessary**. Most important tasks are supported by these tools; these tasks include data integration, transformation, data visualization, and model building. \n",
    "- **KNIME** originated at the University of Konstanz in 2004. As you can see, KNIME has a visual user interface with drag-and-drop capabilities. It also has built-in visualization capabilities. Knime can be be extended by programming in R and Python, and has connectors to Apache Spark. \n",
    "- Another example of this group of tools is **Orange**. It’s less flexible than KNIME, but easier to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
