# 

* [Ноутбук и презентация](https://drive.google.com/drive/folders/1V54OlYH7eFEzFNk7r4ffs63bfA5QOZge)
* [Лекция 2.1: Линейная регрессия](https://youtu.be/khdaLtu9i-s?si=t4COBwJmJzbPTbxI)
* [Лекция 2.2: LogLoss](https://www.youtube.com/watch?v=HV4Bm8UJwIs)
* [Лекция 2.3. Логистическая регрессия]()
* [Лекция 2.4. Градиентный спуск в линейных алгоритмах](https://www.youtube.com/watch?v=N5_VAN1nhm8)
* [Лекция 2.5. Регуляризация в линейной регрессии](https://www.youtube.com/watch?v=L_o8v5A23XA)
* [Лекция 2.6: Нормализация](https://www.youtube.com/watch?v=tOiwAyilk3I)
* [Семинар. Линейная и логистическая регрессия](https://www.youtube.com/watch?v=5VOp5xmBvds) - **хорошее видео**
* [Семинар. Регуляризация в линейной регрессии](https://www.youtube.com/watch?v=56YdhX0xj3k)

## Лекция 2.1: Линейная регрессия
Видео: https://youtu.be/khdaLtu9i-s?si=t4COBwJmJzbPTbxI

00:00 Линейные модели и их применение

**Перессказ от YandexGPT**

* Линейные модели используются в задачах, где данные не слишком сложны и не требуют сложных алгоритмов.
* Линейные модели не переобучаются и просты в применении.

04:22 Линейная регрессия

* Линейная регрессия - это функция, которая зависит от признаков и предсказывает целевые переменные.
* Признаки могут быть неизменёнными исходными, преобразованиями, например степенями, попарными произведениями или полученными с помощью one-hot-encoding.
* Линейная регрессия может быть нелинейной по признакам, но линейной по параметрам.

09:22 Пример линейной регрессии

* Пример линейной регрессии с одним признаком и одной целевой переменной.
* Решение системы уравнений для нахождения весов.
* Обратная матрица может не существовать, например в случае наличия силтных связей меджу 2 признаками - можно в таком случае выкинуть один из признаков или использовать регуляризацию.

13:44 Псевдообратная матрица

* В видео объясняется, что в случае, когда наблюдений больше, чем признаков, нельзя получить обратную матрицу.
* Вместо этого используется псевдообратная матрица, которая определяется для любых матриц и в случае высокой и тонкой матрицы выражается как $(X^TX)^{-1}X^T$.

15:44 Получение решения с помощью производной

* Автор объясняет, как получить решение, используя производную функции потерь.
* Функция потерь определяется как скалярное произведение между признаками и весами, затем вычитается истинное значение целевой переменной и возводится в квадрат.
* Затем берется производная этой функции и приравнивается к нулю.

17:52 Проблемы с вычислением обратной матрицы

* Вычисление обратной матрицы может быть сложным и нестабильным, особенно когда матрица становится очень большой и имеет линейно зависимые столбцы.
* В этом случае элементы обратной матрицы могут быть очень большими по модулю, что делает вычисления нестабильными и может привести к ошибкам в вычислениях.


## Лекция 2.2: LogLoss
Видео: https://www.youtube.com/watch?v=HV4Bm8UJwIs

**Перессказ от Yandex GPT**

00:07 Введение в задачу классификации

* В видео обсуждается задача классификации, где вместо действительных чисел используются нули и единицы.
* Задача классификации - это предсказание вероятности того, что объект принадлежит к определенному классу, учитывая его признаки.

01:06 Функция потерь

* Функция потерь - это мера ошибки предсказания модели.
* В видео предлагается использовать теорему из статистики для выбора функции потерь.
* Функция потерь должна максимизировать правдоподобие, что означает максимизацию вероятности правильного предсказания.

05:55 Логарифм правдоподобия

* Логарифм правдоподобия - это логарифм произведения вероятностей появления каждого элемента в выборке.
* Логарифм правдоподобия можно преобразовать в функцию потерь, которая будет минимизирована.
* Функция потерь будет иметь вид суммы логарифмов вероятностей правильного и неправильного предсказания.

07:50 Заключение

* Задача максимизации правдоподобия и минимизации функции потерь эквивалентны.
* Функция потерь может быть представлена как логарифм правдоподобия.
* В видео обсуждаются различные преобразования функции потерь для упрощения ее вычисления.


## Лекция. Логистическая регрессия
Видео: https://www.youtube.com/watch?v=twdZBko9vH4
**Перессказ от Yandex GPT**
Лекция. Логистическая регрессия

00:08 Введение в логистическую регрессию

* Видео объясняет, как использовать логистическую регрессию для классификации.
* Логистическая регрессия - это модель, которая предсказывает вероятность класса.

01:48 Функция потерь и обучение модели

* Функция потерь для логистической регрессии - это сумма логарифмов вероятностей правильного класса.
* Модель обучается путем минимизации функции потерь.

05:46 Многоклассовая классификация

* Для многоклассовой классификации используются разные веса для каждого класса.
* Вектор логитов возводится в степень, а затем делится на сумму всех значений, чтобы получить вероятности.

07:59 Применение софт-макса

* Функция софт-макс применяется для получения вероятностей из логитов.
* Функция софт-макса похожа на мягкий максимум, но не является дифференцируемой.

11:51 Предсказание вероятностей

* Предсказанная вероятность класса - это элемент вектора вероятностей, возведенный в степень соответствующего логита и деленный на сумму всех элементов вектора.



## Лекция. Градиентный спуск в линейных алгоритмах

Видео: https://www.youtube.com/watch?v=N5_VAN1nhm8
**Перессказ от Yandex GPT**
Лекция. Градиентный спуск в линейных алгоритмах

00:08 Градиентный спуск

* Градиентный спуск - метод нахождения минимума функции, который минимизирует функцию потерь.
* Для нахождения минимума функции, нужно найти производную функции и двигаться в направлении, в котором функция уменьшается.

04:02 Математическое описание градиентного спуска

* Градиент - вектор частных производных функции.
* Альфа - параметр, отвечающий за размер шагов.
* Формула обновления весов: старые веса минус альфа умноженное на градиент в текущей точке.

07:53 Градиентный спуск для параболы

* Пример работы градиентного спуска для параболы.
* Вектор весов уменьшается до нуля.

08:52 Градиентный спуск для линейной регрессии

* Пример градиентного спуска для линейной регрессии.
* Вектор весов обновляется до тех пор, пока не перестанет изменяться.

10:50 Градиентный спуск для логистической регрессии

* Пример градиентного спуска для логистической регрессии.
* Вектор весов обновляется до тех пор, пока не перестанет изменяться.


## Лекция. Регуляризация в линейной регрессии
Видео: https://www.youtube.com/watch?v=L_o8v5A23XA
**Перессказ от Yandex GPT**

00:07 Регуляризация в машинном обучении

* Регуляризация - это метод, который позволяет упростить структуру модели и избежать сложных зависимостей.
* В линейной регрессии, переобучение может произойти из-за линейной зависимости между признаками и весами.

01:04 Линейная зависимость и регуляризация

* Линейная зависимость возникает, когда все признаки умножаются на один и тот же вектор весов, что приводит к нулевому предсказанию.
* Регуляризация помогает избежать этой проблемы, добавляя к функции потерь квадраты весов.

03:01 Градиентный спуск и регуляризация

* Градиентный спуск используется для минимизации эмпирического риска, и регуляризация добавляется к функции потерь для контроля над весами.
* Константа бета позволяет балансировать между уменьшением веса и увеличением качества предсказаний.
* С каждым шагом веса будут уменьшаться, и только потом к ним будет применяться градиентный шаг.




## Лекция 2.6: Нормализация
Видео: https://www.youtube.com/watch?v=tOiwAyilk3I

00:06 Нормализация признаков

* Нормализация признаков - это преобразование данных, при котором из всех элементов в колонке вычитается среднее и делится на стандартное отклонение.
* Это делается для того, чтобы все признаки имели одинаковый масштаб, что улучшает работу методов машинного обучения, таких как градиентный спуск.

02:05 Влияние масштаба признаков на веса

* Разные масштабы признаков могут привести к тому, что веса будут иметь разные масштабы, что может привести к проблемам с регуляризацией.
* Важно использовать нормированные данные на входе нейронной сети, чтобы избежать этих проблем.

03:02 Заключение

* В заключение, автор призывает зрителей прийти на третью лекцию, где будут обсуждаться деревья решений, бустинг и композиции алгоритмов.
* Также будет обсуждаться, как выбирать хорошие модели машинного обучения и как их создавать.

## Семинар. Линейная и логистическая регрессия
Видео: https://www.youtube.com/watch?v=5VOp5xmBvds

**Перессказ от YandexGPT**

Семинар. Линейная и логистическая регрессия

00:07 Линейная регрессия

* Обсуждение линейной регрессии и методов оптимизации и регулирования.
* Обучение модели с использованием градиентных методов.

13:36 Градиентный спуск

* Метод градиентного спуска для нахождения минимума функции потерь.
* Сравнение с аналитическим решением задачи линейной регрессии.

20:16 Статистический градиентный спуск

* Идея статистического градиентного спуска: обработка выборки по частям.
* Пример реализации статистического градиентного спуска на примере линейной регрессии.

21:15 Мини-бач градиентный спуск

* В видео объясняется, как уменьшить размерность при вычислении градиента, используя мини-бач (подвыборку).
* Это позволяет ускорить процесс обучения и уменьшить влияние выбросов на вектор градиента.

27:21 Логистическая регрессия

* В видео рассказывается о задаче классификации, когда вместо предсказания действительных чисел, предсказываются числа из отрезка ноль один.
* Функция потерь и градиент рассчитываются для бинарной классификации.
* Реализуется логистическая регрессия с градиентным спуском.

37:49 Практический пример обучения

* В видео демонстрируется, как логистическая регрессия разделяет классы и как меняются вероятности принадлежности к классам.
* Обсуждаются практические аспекты обучения и регуляризации линейных моделей.

## Семинар. Регуляризация в линейной регрессии


00:06 Введение в регуляризацию
Видео: https://www.youtube.com/watch?v=56YdhX0xj3k

**Перессказ от YandexGPT**

* В линейных моделях могут возникать проблемы с данными, мультиколиниарностью и шумами.
* Регуляризация - метод борьбы с этими проблемами, который заключается в добавлении штрафа за большие веса модели.

02:41 Ридж-регрессия

* Один из способов регуляризации - добавление слагаемого, содержащего или два норму весов.
* Ридж-регрессия реализуется через функцию потерь, в которую добавляется слагаемое с лямда-параметром.
* Проблема с ридж-регрессией - неравномерные штрафы за веса модели из-за разных масштабов признаков.

16:01 Лассо-регрессия

* Другой способ регуляризации - добавление слагаемого, содержащего или один норму весов.
* Лассо-регрессия не имеет аналитического решения, но может быть реализована через статистический ингредиентный спуск.
* Проблема лассо-регрессии - модуль не дифференцируемая функция, что затрудняет поиск аналитического решения.

19:36 Реализация регуляризации

* Реализация регуляризации в виде статистического ингредиентного спуска с лассо-регрессией.
* Результаты регуляризации лучше, чем для ридж-регрессии, но все еще есть некоторые отличия от аналитического решения.

20:30 Регуляризация в логистической регрессии

* В логистической регрессии используется регуляризация для предотвращения нулевых весов и улучшения вероятности принадлежности к классу.
* В случае лассо-регрессии, решение часто будет попадать на углы и вершины квадрата, что приводит к занулению весов.
* Ридж-регрессия, в отличие от лассо, не зануляет веса модели и имеет аналитическое решение.

31:11 Тестирование на реальных данных

* В качестве примера используется набор данных "Iris Fisher", который содержит признаки ирисов и таргет (класс).
* Логистическая регрессия с регуляризацией используется для классификации ирисов на три класса.
* На практике, логистическая регрессия с регуляризацией позволяет избежать проблемы с пограничными объектами, которые имеют почти нулевую или единичную вероятность принадлежности к классу.

