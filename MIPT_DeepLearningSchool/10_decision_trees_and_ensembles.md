# Решающие деревья и композиции алгоритмов
* [Лекция. Решающие деревья](https://www.youtube.com/watch?v=MJwAoWFTMWw)
* [Лекция. Композиции алгоритмов](https://www.youtube.com/watch?v=vqF8wrWjR5s)
* [Лекция. Градиентный бустинг](https://www.youtube.com/watch?v=JElfEE1OrSU)

## Лекция. Решающие деревья

* [Видео](https://www.youtube.com/watch?v=MJwAoWFTMWw)
* [Презентация](https://drive.google.com/file/d/1slRUTiARC7vs9WkkwbFhRRMC90WFaEgd/view?usp=sharing)

**Перессказ от YandexGPT**
00:06 Решающие деревья

* Видео посвящено изучению решающих деревьев, алгоритма машинного обучения, который использует логические правила для принятия решений.
* Решающие деревья представляют собой деревья, где каждая вершина содержит вопрос, на который нужно ответить, и в зависимости от ответа, алгоритм направляется в одну из ветвей дерева.

06:02 Обучение решающего дерева

* Обучение решающего дерева начинается с выбора произвольной вершины дерева и определения количества объектов каждого класса, которые дошли до этой вершины.
* Затем выбирается порог разбиения, который разделяет объекты на две части, чтобы улучшить чистоту вершин.
* Процесс продолжается, пока не будет достигнута терминальная вершина, после чего алгоритм принимает решение о принадлежности объекта к определенному классу.

10:58 Формализация выбора решающего правила

* Обсуждается формализация выбора решающего правила для разбиения вершины, где вероятность нахождения объекта первого класса близка к нулю или единице.
* Вводится функция $H$, которая зависит от вероятности и доли объектов, попавших в левую и правую части.
* Функция $H$ должна быть минимизирована для оптимального разбиения вершины.

14:31 Примеры функций H

* Функция Энтропии: $H(q) = -q \cdot log_2(q) - (1- q)\cdot log_2(1-q)$.
* Индекс джини: $H(q) = 4q \cdot (1-q)$.
* Ошибка классификации: 1 минус ку.

17:23 Выбор оптимального решающего правила

* Рассматриваются простейшие решающие правила вида "признак больше или равен некоторой константе".
* Алгоритмы машинного обучения могут автоматически оптимизировать разбиение вершины, минимизируя функцию $H$.

19:23 Решающие деревья

* Решающие деревья - это алгоритмы машинного обучения, которые используют деревья решений для классификации и регрессии.
* Они могут приблизить почти любую закономерность, если она проявляется только на обучающей уборке.
* Глубина дерева - это параметр, который нужно регулировать для получения оптимального качества.

23:21 Примеры решающих деревьев

* Решающее дерево глубины три не смогло справиться с задачей ириса фишера, но дерево глубины семь уже лучше.
* Дерево глубины двенадцать получилось слишком сложным и хуже работает на тестовой выборке.

27:01 Решающие деревья для регрессии

* Решающие деревья для регрессии также возможны, но работают хуже, чем для классификации.
* В следующих видео будет обсуждаться, как исправить решающие деревья, используя композиции алгоритмов.

Достоинства и недостатки
Достоинства:
1. интерпретируемая модель;
2. простая идея и алгоритм оучения;
3. возможна регуляризация (ограничение глубины, объектов в листах);
4. обработка пропущенных значений.

Недостатки:
1. склонно к переобучению;
2. плохо работает для задач регрессии;
3. сильно изменяется в зависимости от значений параметров.

## Лекция. Композиции алгоритмов

* [Видео](https://www.youtube.com/watch?v=vqF8wrWjR5s)

**Перессказ от YandexGPT**

Лекция. Композиции алгоритмов

00:06 Композиции алгоритмов машинного обучения

* Идея композиции алгоритмов машинного обучения: объединение нескольких простых алгоритмов для получения более мощного алгоритма.
* Композиции алгоритмов основаны на принципе кондорсе и эксперименте Фрэнсиса Гальтона.

05:18 Метод простого голосования

* Метод простого голосования: усреднение ответов нескольких алгоритмов для принятия решения.
* Пример: классификация объектов на основе голосования нескольких решающих деревьев.

07:35 Бутстреп

* Бутстреп: метод генерации новых выборок для обучения алгоритмов машинного обучения.
* Пример: сгенерированные выборки из обучающей выборки для обучения решающих деревьев.

09:36 Бэггинг

* Бэггинг: композиция алгоритмов, основанная на простом голосовании алгоритмов, обученных на случайных бутстрепных выборках.
* Пример: случайный лес, состоящий из сто решающих деревьев, обученных на случайных выборках.

12:45 Решающие леса

* Решающие леса - это композиция алгоритмов, которые обучаются на одной и той же задаче. Обычно это беггинг над решающими деревьями.
* Они могут воспроизвести желаемую спиральную структуру и имеют меньше выбросов, чем решающие деревья большой глубины.

16:45 Стекинг

* Стекинг - это метод, при котором обучаются разноплановые алгоритмы, а затем их ответы используются для обучения нового, более мощного алгоритма.
* Стекинг может быть эффективным, если алгоритмы мощные, но разноплановые.

19:17 Бустинг

* Бустинг - это метод, при котором алгоритмы строятся последовательно и каждый алгоритм исправляет ошибки предыдущих.
* Сильный метод бустинга - градиентный бустинг над решающими деревьями, который будет рассмотрен в следующем видео.

## Лекция. Градиентный бустинг

[Видео](https://www.youtube.com/watch?v=JElfEE1OrSU)

**Перессказ от YandexGPT**

Лекция. Градиентный бустинг

00:06 Градиентный бустинг

* Градиентный бустинг - эффективный способ построения композиции решающих деревьев, где каждое дерево стремится компенсировать ошибку предыдущих деревьев.
* Решение принимается в виде взвешенного голосования.

01:47 Градиентный спуск

* Градиентный спуск - алгоритм минимизации функции одного аргумента.
* Скорость обучения (альфа) может быть постоянной или уменьшаться со временем.

06:03 Задача регрессии

* Задача регрессии: построить алгоритм, который принимает на вход иксы и выдает игрики, оптимизирующие функцию потерь.
* Функция потерь может быть квадратичной или другой, главное, чтобы она была дифференцируемой.

09:04 Градиентный бустинг для регрессии

* Градиентный бустинг используется для решения задачи регрессии.
* Каждый алгоритм вносит существенный вклад в общую сумму, что позволяет компенсировать ошибки предыдущих деревьев.

10:08 Алгоритм градиентного бустинга

* В видео объясняется алгоритм градиентного бустинга, который исправляет ошибки алгоритмов.
* Функция потерь записывается в виде суммы функций потерь предыдущих алгоритмов и нового алгоритма.
* Для минимизации функции потерь используется градиентный спуск.

16:04 Примеры и реализация градиентного бустинга

* В примере используется функция потерь MSE (среднеквадратичная ошибка).
* Градиентный бустинг настраивает новое решающее дерево на разность между желаемым ответом и ответом предыдущих алгоритмов.
* В качестве базовых алгоритмов используются решающие деревья, которые быстро обучаются и легковесны.
* В видео упоминаются три библиотеки для реализации градиентного бустинга: XGBoost, LightGBM и CatBoost от Яндекса.

21:53 Анализ градиентного бустинга

* Градиентный бустинг позволяет точно восстанавливать искомую функцию на обучающей выборке и почти не переобучается на тестовой выборке.
* Бустинг способен компенсировать систематическую ошибку, накапливаемую за счет ошибки каждого из базовых алгоритмов.

25:13 Преимущества и недостатки бустинга

* Преимущества: точное приближение функции, компенсация систематической ошибки, способность работать с произвольной функцией потерь.
* Недостатки: медленный, слабая интерпретируемость, переобучение при избыточном количестве базовых алгоритмов, требуется большое количество обучающих данных.




