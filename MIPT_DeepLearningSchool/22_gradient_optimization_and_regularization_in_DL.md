# Лекция. Продвинутое обучение нейронных сетей

* Видео 1: https://www.youtube.com/watch?v=6CvpMOO-DB4
* Видео 2
* Презентация:https://drive.google.com/file/d/1RGMSApkyItqtDjzVqypffVW05zj0O-bi/view


**Краткое содержание от YaGPT**

## Градиентная оптимизация в Deep Learning

00:07 Повторение основных понятий глубокого обучения

* Обсуждение функций активации, таких как сигмоида, гиперболический тангенс, ReLU и условная ELU.
* Обсуждение механизма обратного распространения ошибки и его важности в глубоком обучении.

01:41 Оптимизация в глубоком обучении

* Обсуждение различных оптимизаторов, таких как градиентный спуск и его модификации.
* Обсуждение важности нормировки данных для улучшения работы моделей.

02:10 Регуляризация в глубоком обучении

* Обсуждение различных техник регуляризации, таких как ограничение на решение, структуру модели и данные.
* Объяснение, почему эти техники можно рассматривать как техники регуляризации.

07:05 Методы доработки градиентного спуска

* Видео обсуждает различные методы доработки градиентного спуска, включая использование momentum, Nesterov momentum (приводятся аналогии с движением шарика с горки, который благодаря инерции может перескакивать небольшие неровности).
* Эти методы помогают сделать градиентный спуск более устойчивым, эффективным и быстрым.

09:58 Проблема шума в градиентном спуске

* Градиентный спуск может быть шумным из-за использования небольшой выборки объектов для обучения.
* Это может привести к тому, что параметры модели будут меняться слишком сильно, что может быть проблемой.

12:45 Использование момента импульса для улучшения градиентного спуска

* Момент импульса или инерция может помочь сделать градиентный спуск более устойчивым и эффективным.
* Это связано с тем, что градиентный спуск можно воспринимать как процесс скатывания шарика вниз, и момент импульса помогает ему преодолевать неровности на поверхности.

13:45 Использование момента для оптимизации параметров модели

* В видео обсуждается использование момента для оптимизации параметров модели.
* Момент - это накопленный импульс с предыдущих шагов, который используется для определения направления движения на текущем шаге.
* Это позволяет модели быстрее сходиться и быть более устойчивой.

15:45 Нестеров момент

* Нестеров момент - это метод, который сначала сдвигает модель по инерции, а затем считает градиент.
* Это позволяет избежать проблем с неустойчивым градиентом и локальными минимумами.

18:26 Сравнение различных методов оптимизации

* В видео сравниваются различные методы оптимизации, включая момент и Нестеров момент.
* Они показывают разные траектории движения и сходятся с разной скоростью.
* Обсуждаются также другие методы, такие как Adagrad, Adadelta, RMSprop, которые также показывают хорошие результаты.

19:38 Адаптивный градиентный спуск

* Видео обсуждает адаптивный градиентный спуск, который автоматически подбирает признаки для каждого параметра.
* Обсуждается идея накопления импульса и использования кэша для адаптации к изменениям градиента.

23:31 Экспоненциальное сглаживание

* Видео объясняет, как использовать экспоненциальное сглаживание для учета последних значений градиента.
* Это позволяет учитывать значение градиента в текущий момент времени и в предыдущие моменты.

27:21 Adam (Adaptive momentum)

* Adam - это комбинация двух подходов, описанных ранее: накопление импульса (Momentum) и использование кэша для адаптации (RMSprop normalization).
* Adam является стандартным оптимизатором в большинстве нейронных сетей и рекомендуется для использования.

29:36 Градиентный спуск и выбор оптимального лерни крейта

* Adam - оптимизатор, который на каждом шаге нормирует себя на то, что не нужно идти куда-то, и в результате получается более эффективный спуск градиента.
* Андрей Карпатый - ученый, который написал в твиттере, что три на десять минус четвертый - хороший learning rate для Adam.
* Выбор оптимального лерни крейта - задача исследователя, и зависит от задачи, модели, данных и шага оптимизации.

31:12 Техника понижения лерни крейта

* Одна из техник понижения лерни крейта - это понижение его в два или десять раз каждые пятьдесят операций.
* Это позволяет модели делать более аккуратные шаги и может привести к снижению функции потерь.

33:58 Выводы и рекомендации

* Если не знаете оптимизатор, лучше использовать Adam.
* Если не знаете learning rate, можно попробовать три на десять минус четвертой (3e-4 это для обольших задач). Для более маленьких - 3e-2. Чем проще задача, тем больше можно делать learning rate.
* Если задача типа классификации, лучше использовать три на десять минус четыре.
* Использовать learning rate decay.
* Важно следить за качеством модели, используя графики функции потерь на разных выборках (обучающей и валидационной, на валидационной в конце каждой эпохи).

## Регуляризация в Deep Learning

00:07 Регуляризация в глубоком обучении

* Регуляризация - это техника, которая позволяет использовать модель более эффективно, внедряя внутреннее понимание задачи в процесс обучения.
* В линейных моделях регуляризация может быть использована для ограничения весов, что приводит к отбору признаков.

01:51 Нормировка данных

* Нормировка данных может быть использована для улучшения качества модели и предотвращения переобучения.
* Нормировка данных включает вычитание среднего и деление на дисперсию.
* Нормировка данных полезна для логистической регрессии, линейной регрессии (при условии последующего применения регуляризации) и метода главных компонент.

05:37 Нормирование данных

* В видео обсуждается необходимость нормализации данных для линейных моделей.
* Нормирование данных также важно для нейронных сетей, так как они используют градиенты для обновления параметров.

09:33 Бачнорм и его применение

* Бачнорм (batch normalization) - это техника, которая появилась в 2015 году и позволяет нормировать данные на входе в модели.
* Бачнорм вычитает среднее значение и делит на дисперсию по каждому признаку, что приводит к нулевому среднему и единичной дисперсии.

11:35 Нормирование данных и регуляризация

* В видео обсуждается техника нормализации данных, которая позволяет модели учиться быстрее и стабильнее.
* Нормирование данных включает в себя использование экспоненциального сглаживания для усреднения и дисперсии.

18:49 Техника регуляризации

* Регуляризация используется для ограничения веса модели и предотвращения переобучения.
* Техника регуляризации включает в себя использование нормы вектора весов и ограничение на значения весов.

19:45 Техника dropout

* Техника дропаут предлагает случайным образом занулять некоторые значения признаков, чтобы модель работала только на частично признаковом описании.
* Это позволяет модели учиться на неполных данных и предотвращает переобучение.

21:10 Обучение с использованием дропаутов

* В видео обсуждается пример обучения модели, где каждый из 31 работника выполняет свою задачу, но не умеет работать с другими типами овощей.
* Если один из работников заболеет, это приведет к остановке процесса, так как остальные не смогут выполнить свою работу.

22:10 Естественный пример дропаутов

* В качестве примера используется коллектив по разработке программного обеспечения, где один из сотрудников рисовал кнопки.
* Если он заболеет, остальные сотрудники смогут выполнить его работу, но с меньшим качеством.

23:09 Техника дропаутов

* В процессе обучения модели, каждый признак может быть занутлен случайным образом, что делает модель более устойчивой к аномальным значениям признаков и переобучению.
* Эта техника может принимать различные формы, включая использование дропаутов в языковом моделировании.

24:09 Переносимость дропаутов

* При переходе из тренировочного в тестовый режим, данные должны быть перенормированы, чтобы сохранить распределение.
* Дропаут может рассматриваться как ансамблирование множества сетей, что делает его похожим на методы ансамблирования.

27:03 Регуляризация и аугментация данных

* В видео обсуждается регуляризация и аугментация данных в контексте машинного обучения.
* Регуляризация - это способ добавить преобразования в данные, которые сохраняют их смысл.
* Аугментация данных - это простой способ регуляризации, который добавляет преобразования, которые не меняют значение целевой переменной.
* Это увеличивает объем данных и делает модель более устойчивой к преобразованиям.

28:59 Влияние регуляризации на обобщающую способность модели

* Регуляризация может снизить обобщающую способность модели, если использовать слишком много преобразований.
* Важно использовать регуляризацию в разумных пределах и не перебарщивать.

31:54 Вывод

* Оптимизационные методы, такие как градиентные методы, являются ключевыми в машинном обучении.
* Важно использовать их с умом и обращать внимание на правила большого пальца, такие как начальная инициализация и подбор параметров.
* Регуляризация - это внесение априорных знаний в процесс обучения модели и использование их для улучшения результатов.
* Аугментация данных, например, в компьютерном зрении, является стандартом и используется в 90% случаев.
* Регуляризация и аргументация данных могут быть использованы для улучшения результатов и устойчивости модели.

