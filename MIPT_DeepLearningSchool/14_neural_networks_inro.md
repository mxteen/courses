# Введение в нейронные сети

* [История развития Deep Learning](https://www.youtube.com/watch?v=ZfXpX8tMg-w)
* [Механизм обратного распространения ошибки](https://www.youtube.com/watch?v=-yiq1DRX9K0)
* [Лекция. Полносвязная нейронная сеть](https://www.youtube.com/watch?v=O0nGKKFyYT4)
* [Функции активации. Краткий обзор применений CNN и RNN](https://www.youtube.com/watch?v=3F7rydcAa0w)

## Введение в нейронные сети. Часть 1. История развития Deep Learning

Видео: https://www.youtube.com/watch?v=ZfXpX8tMg-w

**Перессказ от YandexGPT**

00:06 Введение в искусственные нейронные сети

* Родослав Нечев, автор курсов по машинному обучению, представляет лекцию о искусственных нейронных сетях и глубоком обучении.
* Лекция - историческая справка.

04:15 Историческая справка

* Первая точка на временной шкале истории нейронных сетей относится к сороковым годам (1940).
* В 1943 году учитель и ученик, Маккало и Пиц, представили математическую модель нейронных сетей.
* В 1958 году Фрэнк Розенблатт представил перцептрон, который стал основой для современных нейронных сетей.

06:01 Механизм обратного распространения ошибки

* Обратный процесс ошибки или "back propogation" является краеугольным камнем в обучении нейронных сетей.
* Понимание этого процесса важно для понимания того, как нейронные сети учатся и где их можно применять.

08:10 Функции активации

* В лекции будут рассмотрены различные функции активации, которые используются в нейронных сетях для определения того, как нейроны реагируют на входные данные.
* В конце лекции будет представлен обзор интересных нейронных сетей и игровая площадка для построения нейронных сетей в браузере без кода.

09:19 Перцептрон и его применение

* Перцептрон - математическая модель нейрона, на основе которой была построена система машинного зрения в 60-х годах.
* Система обучалась различать цвета, используя собственный опыт и метод проб и ошибок.

10:19 Критика и смерть Фрэнка Розенблата

* После публикации работы Фрэнка Розенблата, вышла критика, утверждающая, что перцептрон не способен перевернуть мир.
* Фрэнк Розенблат не смог ответить на критику по объективным причинам. Это посспособствовало временной утрате интереса к нейронным сетям.

11:12 Проблема исключающего или и ее решение

* Проблема линейной неразделимости точек, которые можно разделить только с использованием новых признаков.
* Добавление второго слоя нейронной сети позволяет решить проблему с использованием линейных моделей.

13:16 Появление обратного распространения ошибки

* Метод обратного распространения ошибки, предложенный одновременно в двух лагерях, определяет дальнейшее развитие нейронных сетей.
* Появление новых моделей и эйфория от их использования, но затем интерес к ним угасает на 10 лет.

15:05 Развитие нейронных сетей в 2000-х годах

* Нейронные сети продолжают развиваться, появляются новые открытия и идеи, но в академической среде.
* В 2011-2012 годах нейронные сети заявляют о себе на соревновании ImageNet, переворачивая ситуацию в свою пользу.

16:41 История нейронных сетей

* Видео начинается с обзора истории нейронных сетей, начиная с простой сети "AlexNet", которая победила на соревнованиях по классификации изображений.
* "AlexNet" имеет структуру, похожую на "Lent" Яна Ликуна, но с более современными подходами.
* Появление "AlexNet" привлекло внимание к нейронным сетям, но интерес к ним возрос только после накопления достаточного объема данных для обучения.

19:31 Современные достижения

* В настоящее время нейронные сети используются для решения различных задач, включая распознавание объектов, семантическую сегментацию, генерацию подписей к изображениям, смену стиля фотографий, обработку звуков и генерацию изображений.
* Генеративные состязательные сети (GAN) позволяют генерировать изображения и звуки, которые выглядят и звучат реалистично.

22:32 Основы и последние достижения

* В последние годы появилось множество моделей для обработки естественного языка, включая трансформер-модели и модели с большим количеством параметров.
* В видео также упоминается "Bert", который является стандартом де-факто в обработке естественного языка, и GPT2, которая имеет 1,5 миллиарда параметров.
* Область глубокого обучения продолжает развиваться, и в видео обсуждаются различные подходы и модели, которые используются в этой области.

## Введение в нейронные сети. Часть 2. Механизм обратного распространения ошибки
Видео: https://www.youtube.com/watch?v=-yiq1DRX9K0

**Перессказ от YandexGPT**

Введение в нейронные сети. Часть 2. Механизм обратного распространения ошибки

00:05 Введение в нейронные сети

* Видео объясняет, как строить нейронные сети и как они работают.
* Нейронные сети состоят из нескольких слоев, каждый из которых выполняет определенную функцию.
* Первый слой принимает входные данные и преобразует их в линейную комбинацию признаков.
* Второй слой выполняет нелинейное преобразование, используя функцию активации, такую как сигмоида или гиперболический тангенс.
* Третий слой снова преобразует данные, используя линейную комбинацию признаков.

04:58 Обучение нейронной сети

* Обучение нейронной сети происходит с использованием градиентного спуска.
* Каждый слой сети обучается на основе результатов предыдущего слоя.
* В процессе обучения нейронная сеть автоматически подбирает оптимальные параметры и функции активации.
* Важно, чтобы все функции активации были дифференцируемыми, чтобы градиентный спуск мог работать.

06:57 Функции активации

* В видео обсуждаются основные функции активации, такие как сигмоида, гиперболический тангенс и функция максимума.
* Выбор функции активации зависит от задачи и может быть индивидуальным для каждой задачи.

08:54 Слои и функции активации в нейронных сетях

* Видео объясняет, что слои в нейронных сетях представляют собой структурные единицы, включающие линейные преобразования и функции активации.
* Слои могут быть входными, выходными и промежуточными, а также могут быть сверточными или рекуррентными.
* Функции активации применяются к результатам линейных преобразований и могут быть различными, такими как сигмоидальная функция, тангенс и другие.

12:49 Обучение нейронных сетей с помощью обратного распространения ошибки

* Видео объясняет механизм обратного распространения ошибки, который используется для обучения нейронных сетей.
* Обратный процесс начинается с вычисления частных производных функции потерь по параметрам сети.
* Затем эти производные используются для определения направления изменения параметров, чтобы уменьшить значение функции потерь.
* Пример обратного распространения ошибки показан на примере логистической регрессии.

17:43 Производные экспоненты

* Видео объясняет, как дифференцировать экспоненту и производную линейной функции.
* В конце видео автор объясняет, что производная экспоненты равна экспоненте производной линейной функции.

21:37 Матричное представление

* Автор объясняет, как дифференцировать вектор по вектору и матрицу по вектору.
* Он также упоминает, что производная скаляра по скаляру будет скаляром, производная вектора по скаляру будет вектором, а производная вектора по вектору будет матрицей.

24:34 Градиентный спуск

* Автор объясняет, что градиентный спуск используется для обновления параметров модели.
* Он также объясняет, что стохастический градиентный спуск может быть использован, когда выборка объектов слишком велика.
* Автор подчеркивает, что выборка должна быть случайной и не должна нарушать структуру последовательности объектов.

## Лекция. Полносвязная нейронная сеть

Видео: https://www.youtube.com/watch?v=O0nGKKFyYT4

**Перессказ от YandexGPT**

Лекция. Полносвязная нейронная сеть

00:08 Модель нейрона

* В видео обсуждается модель нейрона, которая состоит из тела нейрона, входов, весов и функции активации.
* В качестве функции активации часто используется сигмоида, но также может быть использована функция релу.

05:18 Многослойный перцептрон

* Многослойный перцептрон - это простейшая архитектура нейронной сети, где каждый слой связан со всеми нейронами предыдущего слоя.
* На выходном слое находятся десять нейронов, соответствующих классам, к которым относится изображение.

07:16 Обучение многослойного перцептрона

* Обучение многослойного перцептрона происходит с использованием алгоритма обратного распространения ошибки.
* В процессе обучения нейронная сеть вычисляет скалярное произведение между вектором весов и вектором входных данных.

08:05 Матрицы и векторы

* В видео объясняется, что матрица дубль в состоит из строк, соответствующих весам каждого нейрона выходного слоя.
* Вектор склярных произведений может быть записан как матрица дубль в умножить на вектор икс.

09:40 Описание нейронной сети

* В видео описывается полновязная нейронная сеть, состоящая из трех слоев: полносвязанного, полносвязанного и полносвязанного.
* Параметры сети включают матрицы каждого слоя и три вектора свободных членов.

11:48 Преобразование выходов нейронов

* Для преобразования выходов нейронов на последнем слое в вероятности используется функция активации софт макс.
* После применения функции активации, вероятности могут быть интерпретированы как меры принадлежности класса.

14:19 Обучение нейронной сети

* Обучение нейронной сети заключается в оптимизации произведения вероятностей, максимизируя его для каждого элемента обучающей выборки.
* Для оптимизации используется алгоритм стахастического градиентного спуска и алгоритм бэк-пропагейшн.

19:25 Применение алгоритма бэк-пропагейшн

* Алгоритм бэк-пропагейшн используется для вычисления производной функции потерь по всем весам нейронной сети.
* В видео демонстрируется применение алгоритма к полновязной нейронной сети для задачи классификации.

14:00 Введение

* В видео обсуждается алгоритм обратного распространения ошибки, который используется для обучения нейронных сетей.
* Рассматривается пример с использованием функции активации сигмоиды и градиентного спуска для оптимизации весов.

19:30 Правила производной композиции

* Обсуждаются правила производной композиции для вычисления градиента функции потерь по весам нейронной сети.
* Рассматриваются примеры вычисления градиента для различных слоев нейронной сети.

27:09 Абстракция слоя

* Обсуждается абстракция слоя, которая определяет две вещи: форд- перфоманс (функция, которую реализует слой) и бэкфорд- перфоманс (как градиенты протекают через слой).
* Рассматривается пример линейного слоя и его абстракция.

28:02 Заключение

* В заключение видео подчеркивается важность понимания правил производной композиции и абстракции слоя для успешного обучения нейронных сетей.

28:54 Линейный слой нейронной сети

* Линейный слой нейронной сети имеет вход и выход, а также функцию, связывающую их.
* Функция потерь и градиенты вычисляются для линейного слоя.

36:08 Реализация линейного слоя

* Реализован класс для линейного слоя, который имеет функции форвард и бэкфорт.
* Функция бэкфорт вычисляет градиенты для линейного слоя и весов.

37:58 Бэк-проп для нелинейных слоев

* Для нелинейных слоев, таких как сигмоида, градиенты вычисляются с помощью транспонирования матрицы и умножения на вектор.
* Задача для самостоятельного изучения - понять, как это работает для других функций активации.


## Введение в нейронные сети. Часть 3. Функции активации. Краткий обзор применений CNN и RNN

Видео:

**Перессказ от YandexGPT**

00:06 Обзор функций активации

* Видео обсуждает различные функции активации, которые используются в нейронных сетях для преобразования промежуточных признаковых представлений данных.
* Упоминаются сигмоида, tanh, ReLU (max(0, a)), ELU и другие функции.

01:06 Преимущества и недостатки функций активации

* Сигмоида хорошо интерпретируется, но имеет значительные хвосты и не центрированный выход.
* Гиперболический тангенс имеет центрированный выход, но также затухание градиентов и необходимость считать экспоненту.
* ReLU и ULU имеют более центрированный выход и не затухают градиенты, но требуют экспоненты для расчета производной.

06:58 Выбор функции активации

* Функция активации должна подходить под решаемую задачу, поэтому выбор зависит от контекста.
* Сигмоиды могут использоваться для получения вероятности, ReLU и ULU могут быть использованы для более гладкой функции активации.
* Если нет ограничений на значение выхода, можно использовать tanh.

09:44 Рекурентные нейронные сети

* Рекурентные нейронные сети могут генерировать тексты, включая поэзию Шекспира и Пушкина.
* Рекурентные сети могут генерировать код на различных языках, включая C++ и Python.

15:40 Сверточные нейронные сети

* Сверточные нейронные сети во многом похожи на работу зрительной коры.
* Сверточные сети могут автоматически обучаться на визуальных стимулах, таких как палочки и точки.

18:09 Работа с нейронными сетями

* Рекомендуется попробовать решить задачу с помощью нейронных сетей в интерфейсе [TensorFlow PlayGround](http://playground.tensorflow.org).

19:20 Итоги и рекомендации

* Автор поздравляет зрителей с вхождением в область искусственного интеллекта и глубокого обучения.
* Он подчеркивает, что нейронные сети - это замечательные методы, которые позволяют решать сложные задачи, но для этого необходимо правильно выбирать наборы данных и структуру сетей.
* Он также отмечает, что на данный момент нейронные сети должны быть полностью дифференцируемыми, чтобы можно было оптимизировать их параметры.
* Автор призывает зрителей не стесняться смотреть вокруг себя и искать вдохновение в природе и физике, а также не бояться задавать вопросы и изучать новое.

