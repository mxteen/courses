# 

* [Ноутбук и презентация](https://drive.google.com/drive/folders/1V54OlYH7eFEzFNk7r4ffs63bfA5QOZge)
* [Лекция 2.1: Линейная регрессия](https://youtu.be/khdaLtu9i-s?si=t4COBwJmJzbPTbxI)
* [Лекция 2.2: LogLoss](https://www.youtube.com/watch?v=HV4Bm8UJwIs)
* [Лекция 2.3. Логистическая регрессия]()
* [Лекция 2.4. Градиентный спуск в линейных алгоритмах](https://www.youtube.com/watch?v=N5_VAN1nhm8)
* [Лекция 2.5. Регуляризация в линейной регрессии](https://www.youtube.com/watch?v=L_o8v5A23XA)
* [Лекция 2.6: Нормализация](https://www.youtube.com/watch?v=tOiwAyilk3I)

## Лекция 2.1: Линейная регрессия
Видео: https://youtu.be/khdaLtu9i-s?si=t4COBwJmJzbPTbxI

00:00 Линейные модели и их применение

* Линейные модели используются в задачах, где данные не слишком сложны и не требуют сложных алгоритмов.
* Линейные модели не переобучаются и просты в применении.

04:22 Линейная регрессия

* Линейная регрессия - это функция, которая зависит от признаков и предсказывает целевые переменные.
* Признаки могут быть изначальными, преобразованиями, степенями, попарными произведениями и ван хотинга.
* Линейная регрессия может быть нелинейной по признакам, но линейной по параметрам.

09:22 Пример линейной регрессии

* Пример линейной регрессии с одним признаком и одной целевой переменной.
* Решение системы уравнений для нахождения весов.
* Обратная матрица может не существовать, в этом случае используется регуляризация.

13:44 Псевдообратная матрица

* В видео объясняется, что в случае, когда наблюдений больше, чем признаков, нельзя взять обратную матрицу.
* Вместо этого используется псевдообратная матрица, которая определяется для любых матриц и в случае высокой и тонкой матрицы выражается как транспонированная матрица, умноженная на саму себя.

15:44 Получение решения с помощью производной

* Автор объясняет, как получить решение, используя производную функции потерь.
* Функция потерь определяется как скалярное произведение между признаками и весами, затем вычитается истинное значение целевой переменной и возводится в квадрат.
* Затем берется производная этой функции и приравнивается к нулю.

17:52 Проблемы с вычислением обратной матрицы

* Вычисление обратной матрицы может быть сложным и нестабильным, особенно когда матрица становится очень большой и имеет линейно зависимые столбцы.
* В этом случае элементы обратной матрицы могут быть очень большими по модулю, что делает вычисления нестабильными и может привести к ошибкам в вычислениях.


## Лекция 2.2: LogLoss
Видео: https://www.youtube.com/watch?v=HV4Bm8UJwIs

**Перессказ от Yandex GPT**

00:07 Введение в задачу классификации

* В видео обсуждается задача классификации, где вместо действительных чисел используются нули и единицы.
* Задача классификации - это предсказание вероятности того, что объект принадлежит к определенному классу, учитывая его признаки.

01:06 Функция потерь

* Функция потерь - это мера ошибки предсказания модели.
* В видео предлагается использовать теорему из статистики для выбора функции потерь.
* Функция потерь должна максимизировать правдоподобие, что означает максимизацию вероятности правильного предсказания.

05:55 Логарифм правдоподобия

* Логарифм правдоподобия - это логарифм произведения вероятностей появления каждого элемента в выборке.
* Логарифм правдоподобия можно преобразовать в функцию потерь, которая будет минимизирована.
* Функция потерь будет иметь вид суммы логарифмов вероятностей правильного и неправильного предсказания.

07:50 Заключение

* Задача максимизации правдоподобия и минимизации функции потерь эквивалентны.
* Функция потерь может быть представлена как логарифм правдоподобия.
* В видео обсуждаются различные преобразования функции потерь для упрощения ее вычисления.


## Лекция. Логистическая регрессия
Видео: https://www.youtube.com/watch?v=twdZBko9vH4
**Перессказ от Yandex GPT**
Лекция. Логистическая регрессия

00:08 Введение в логистическую регрессию

* Видео объясняет, как использовать логистическую регрессию для классификации.
* Логистическая регрессия - это модель, которая предсказывает вероятность класса.

01:48 Функция потерь и обучение модели

* Функция потерь для логистической регрессии - это сумма логарифмов вероятностей правильного класса.
* Модель обучается путем минимизации функции потерь.

05:46 Многоклассовая классификация

* Для многоклассовой классификации используются разные веса для каждого класса.
* Вектор логитов возводится в степень, а затем делится на сумму всех значений, чтобы получить вероятности.

07:59 Применение софт-макса

* Функция софт-макс применяется для получения вероятностей из логитов.
* Функция софт-макса похожа на мягкий максимум, но не является дифференцируемой.

11:51 Предсказание вероятностей

* Предсказанная вероятность класса - это элемент вектора вероятностей, возведенный в степень соответствующего логита и деленный на сумму всех элементов вектора.



## Лекция. Градиентный спуск в линейных алгоритмах

Видео: https://www.youtube.com/watch?v=N5_VAN1nhm8
**Перессказ от Yandex GPT**
Лекция. Градиентный спуск в линейных алгоритмах

00:08 Градиентный спуск

* Градиентный спуск - метод нахождения минимума функции, который минимизирует функцию потерь.
* Для нахождения минимума функции, нужно найти производную функции и двигаться в направлении, в котором функция уменьшается.

04:02 Математическое описание градиентного спуска

* Градиент - вектор частных производных функции.
* Альфа - параметр, отвечающий за размер шагов.
* Формула обновления весов: старые веса минус альфа умноженное на градиент в текущей точке.

07:53 Градиентный спуск для параболы

* Пример работы градиентного спуска для параболы.
* Вектор весов уменьшается до нуля.

08:52 Градиентный спуск для линейной регрессии

* Пример градиентного спуска для линейной регрессии.
* Вектор весов обновляется до тех пор, пока не перестанет изменяться.

10:50 Градиентный спуск для логистической регрессии

* Пример градиентного спуска для логистической регрессии.
* Вектор весов обновляется до тех пор, пока не перестанет изменяться.


## Лекция. Регуляризация в линейной регрессии
Видео: https://www.youtube.com/watch?v=L_o8v5A23XA
**Перессказ от Yandex GPT**

00:07 Регуляризация в машинном обучении

* Регуляризация - это метод, который позволяет упростить структуру модели и избежать сложных зависимостей.
* В линейной регрессии, переобучение может произойти из-за линейной зависимости между признаками и весами.

01:04 Линейная зависимость и регуляризация

* Линейная зависимость возникает, когда все признаки умножаются на один и тот же вектор весов, что приводит к нулевому предсказанию.
* Регуляризация помогает избежать этой проблемы, добавляя к функции потерь квадраты весов.

03:01 Градиентный спуск и регуляризация

* Градиентный спуск используется для минимизации эмпирического риска, и регуляризация добавляется к функции потерь для контроля над весами.
* Константа бета позволяет балансировать между уменьшением веса и увеличением качества предсказаний.
* С каждым шагом веса будут уменьшаться, и только потом к ним будет применяться градиентный шаг.




## Лекция 2.6: Нормализация
Видео: https://www.youtube.com/watch?v=tOiwAyilk3I

Лекция 2.6: Нормализация.

00:06 Нормализация признаков

* Нормализация признаков - это преобразование данных, при котором из всех элементов в колонке вычитается среднее и делится на стандартное отклонение.
* Это делается для того, чтобы все признаки имели одинаковый масштаб, что улучшает работу методов машинного обучения, таких как градиентный спуск.

02:05 Влияние масштаба признаков на веса

* Разные масштабы признаков могут привести к тому, что веса будут иметь разные масштабы, что может привести к проблемам с регуляризацией.
* Важно использовать нормированные данные на входе нейронной сети, чтобы избежать этих проблем.

03:02 Заключение

* В заключение, автор призывает зрителей прийти на третью лекцию, где будут обсуждаться деревья решений, бустинг и композиции алгоритмов.
* Также будет обсуждаться, как выбирать хорошие модели машинного обучения и как их создавать.

